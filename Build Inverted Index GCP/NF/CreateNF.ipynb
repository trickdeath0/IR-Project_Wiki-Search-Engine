{"cells":[{"cell_type":"markdown","metadata":{"id":"ARADFd6_q9gU"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9WhOUPa7q38q"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lZE1am56q_ya"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WxzwuRyyrBey"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan 13 19:58 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-7789-m.c.ir-ass3-84763.internal:40459\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7feffd101550>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*\n","\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n","spark"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"uEZxySThrFJC"},"outputs":[{"name":"stdout","output_type":"stream","text":["gs://project_ir_os/multistream10_preprocessed.parquet\n","gs://project_ir_os/multistream11_part2_preprocessed.parquet\n","gs://project_ir_os/multistream11_preprocessed.parquet\n","gs://project_ir_os/multistream12_part2_preprocessed.parquet\n","gs://project_ir_os/multistream12_preprocessed.parquet\n","gs://project_ir_os/multistream13_part2_preprocessed.parquet\n","gs://project_ir_os/multistream13_preprocessed.parquet\n","gs://project_ir_os/multistream14_part2_preprocessed.parquet\n","gs://project_ir_os/multistream14_preprocessed.parquet\n","gs://project_ir_os/multistream15_part2_preprocessed.parquet\n","gs://project_ir_os/multistream15_part3_preprocessed.parquet\n","gs://project_ir_os/multistream15_preprocessed.parquet\n","gs://project_ir_os/multistream16_part2_preprocessed.parquet\n","gs://project_ir_os/multistream16_part3_preprocessed.parquet\n","gs://project_ir_os/multistream16_preprocessed.parquet\n","gs://project_ir_os/multistream17_part2_preprocessed.parquet\n","gs://project_ir_os/multistream17_part3_preprocessed.parquet\n","gs://project_ir_os/multistream17_preprocessed.parquet\n","gs://project_ir_os/multistream18_part2_preprocessed.parquet\n","gs://project_ir_os/multistream18_part3_preprocessed.parquet\n","gs://project_ir_os/multistream18_preprocessed.parquet\n","gs://project_ir_os/multistream19_part2_preprocessed.parquet\n","gs://project_ir_os/multistream19_part3_preprocessed.parquet\n","gs://project_ir_os/multistream19_preprocessed.parquet\n","gs://project_ir_os/multistream1_preprocessed.parquet\n","gs://project_ir_os/multistream20_part2_preprocessed.parquet\n","gs://project_ir_os/multistream20_part3_preprocessed.parquet\n","gs://project_ir_os/multistream20_preprocessed.parquet\n","gs://project_ir_os/multistream21_part2_preprocessed.parquet\n","gs://project_ir_os/multistream21_part3_preprocessed.parquet\n","gs://project_ir_os/multistream21_preprocessed.parquet\n","gs://project_ir_os/multistream22_part2_preprocessed.parquet\n","gs://project_ir_os/multistream22_part3_preprocessed.parquet\n","gs://project_ir_os/multistream22_part4_preprocessed.parquet\n","gs://project_ir_os/multistream22_preprocessed.parquet\n","gs://project_ir_os/multistream23_part2_preprocessed.parquet\n","gs://project_ir_os/multistream23_part3_preprocessed.parquet\n","gs://project_ir_os/multistream23_part4_preprocessed.parquet\n","gs://project_ir_os/multistream23_preprocessed.parquet\n","gs://project_ir_os/multistream24_part2_preprocessed.parquet\n","gs://project_ir_os/multistream24_part3_preprocessed.parquet\n","gs://project_ir_os/multistream24_part4_preprocessed.parquet\n","gs://project_ir_os/multistream24_part5_preprocessed.parquet\n","gs://project_ir_os/multistream24_preprocessed.parquet\n","gs://project_ir_os/multistream25_part2_preprocessed.parquet\n","gs://project_ir_os/multistream25_part3_preprocessed.parquet\n","gs://project_ir_os/multistream25_part4_preprocessed.parquet\n","gs://project_ir_os/multistream25_preprocessed.parquet\n","gs://project_ir_os/multistream26_preprocessed.parquet\n","gs://project_ir_os/multistream27_part2_preprocessed.parquet\n","gs://project_ir_os/multistream27_part3_preprocessed.parquet\n","gs://project_ir_os/multistream27_preprocessed.parquet\n","gs://project_ir_os/multistream2_preprocessed.parquet\n","gs://project_ir_os/multistream3_preprocessed.parquet\n","gs://project_ir_os/multistream4_preprocessed.parquet\n","gs://project_ir_os/multistream5_preprocessed.parquet\n","gs://project_ir_os/multistream6_preprocessed.parquet\n","gs://project_ir_os/multistream7_preprocessed.parquet\n","gs://project_ir_os/multistream8_preprocessed.parquet\n","gs://project_ir_os/multistream9_preprocessed.parquet\n"]}],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'project_ir_os' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and \"multistream\" in b.name and \".parquet\" in b.name:\n","        paths.append(full_path+b.name)\n","        print(full_path+b.name)\n","#     if b.name != 'graphframes.sh':\n","#         paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"fAfHYlafrKWS"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"B6srZ5VBrTaY"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","import math\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords) #to remark\n","\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","def tokenize(text):\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","\n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.\n","\n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if\n","                      token.group() not in all_stopwords]\n","    return list_of_tokens\n","\n","\n","def doc_length(text,id1):\n","    tokens = tokenize(text)\n","    d={}\n","    s=0\n","    for token in tokens:\n","        d[token]=d.get(token,0)+1\n","    for k,v in d.items():\n","        s+=v*v\n","    q=math.sqrt(s)\n","    return id1,q\n","\n","lens=doc_text_pairs.map(lambda x:doc_length(x[0],x[1]))\n","dict_len=lens.collectAsMap()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sl3sN18erfYZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://nf.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 84.7 MiB/ 84.7 MiB]                                                \n","Operation completed over 1 objects/84.7 MiB.                                     \n"]}],"source":["import pickle\n","# Open a file to write the pickle to\n","\n","index_src = \"nf.pkl\"\n","with open(index_src, \"wb\") as pickle_file:\n","    # Write the dictionary to the file\n","    pickle.dump(dict_len, pickle_file)\n","    \n","index_dst = f'gs://{bucket_name}/NF/{index_src}' # TODO::\n","!gsutil cp $index_src $index_dst\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":1}