{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"a00e032c"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"5ac36d3a","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-005b  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":2,"id":"d8f56ecd","metadata":{"id":"d8f56ecd","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":3,"id":"38a897f2","metadata":{"id":"38a897f2","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan 10 12:21 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":4,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":5,"id":"72bed56b","metadata":{"id":"72bed56b","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-e0f6-m.c.ir-ass3-84763.internal:33331\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f3e2c7ff4c0>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":6,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["gs://project_ir_os/multistream10_preprocessed.parquet\n","gs://project_ir_os/multistream11_part2_preprocessed.parquet\n","gs://project_ir_os/multistream11_preprocessed.parquet\n","gs://project_ir_os/multistream12_part2_preprocessed.parquet\n","gs://project_ir_os/multistream12_preprocessed.parquet\n","gs://project_ir_os/multistream13_part2_preprocessed.parquet\n","gs://project_ir_os/multistream13_preprocessed.parquet\n","gs://project_ir_os/multistream14_part2_preprocessed.parquet\n","gs://project_ir_os/multistream14_preprocessed.parquet\n","gs://project_ir_os/multistream15_part2_preprocessed.parquet\n","gs://project_ir_os/multistream15_part3_preprocessed.parquet\n","gs://project_ir_os/multistream15_preprocessed.parquet\n","gs://project_ir_os/multistream16_part2_preprocessed.parquet\n","gs://project_ir_os/multistream16_part3_preprocessed.parquet\n","gs://project_ir_os/multistream16_preprocessed.parquet\n","gs://project_ir_os/multistream17_part2_preprocessed.parquet\n","gs://project_ir_os/multistream17_part3_preprocessed.parquet\n","gs://project_ir_os/multistream17_preprocessed.parquet\n","gs://project_ir_os/multistream18_part2_preprocessed.parquet\n","gs://project_ir_os/multistream18_part3_preprocessed.parquet\n","gs://project_ir_os/multistream18_preprocessed.parquet\n","gs://project_ir_os/multistream19_part2_preprocessed.parquet\n","gs://project_ir_os/multistream19_part3_preprocessed.parquet\n","gs://project_ir_os/multistream19_preprocessed.parquet\n","gs://project_ir_os/multistream1_preprocessed.parquet\n","gs://project_ir_os/multistream20_part2_preprocessed.parquet\n","gs://project_ir_os/multistream20_part3_preprocessed.parquet\n","gs://project_ir_os/multistream20_preprocessed.parquet\n","gs://project_ir_os/multistream21_part2_preprocessed.parquet\n","gs://project_ir_os/multistream21_part3_preprocessed.parquet\n","gs://project_ir_os/multistream21_preprocessed.parquet\n","gs://project_ir_os/multistream22_part2_preprocessed.parquet\n","gs://project_ir_os/multistream22_part3_preprocessed.parquet\n","gs://project_ir_os/multistream22_part4_preprocessed.parquet\n","gs://project_ir_os/multistream22_preprocessed.parquet\n","gs://project_ir_os/multistream23_part2_preprocessed.parquet\n","gs://project_ir_os/multistream23_part3_preprocessed.parquet\n","gs://project_ir_os/multistream23_part4_preprocessed.parquet\n","gs://project_ir_os/multistream23_preprocessed.parquet\n","gs://project_ir_os/multistream24_part2_preprocessed.parquet\n","gs://project_ir_os/multistream24_part3_preprocessed.parquet\n","gs://project_ir_os/multistream24_part4_preprocessed.parquet\n","gs://project_ir_os/multistream24_part5_preprocessed.parquet\n","gs://project_ir_os/multistream24_preprocessed.parquet\n","gs://project_ir_os/multistream25_part2_preprocessed.parquet\n","gs://project_ir_os/multistream25_part3_preprocessed.parquet\n","gs://project_ir_os/multistream25_part4_preprocessed.parquet\n","gs://project_ir_os/multistream25_preprocessed.parquet\n","gs://project_ir_os/multistream26_preprocessed.parquet\n","gs://project_ir_os/multistream27_part2_preprocessed.parquet\n","gs://project_ir_os/multistream27_part3_preprocessed.parquet\n","gs://project_ir_os/multistream27_preprocessed.parquet\n","gs://project_ir_os/multistream2_preprocessed.parquet\n","gs://project_ir_os/multistream3_preprocessed.parquet\n","gs://project_ir_os/multistream4_preprocessed.parquet\n","gs://project_ir_os/multistream5_preprocessed.parquet\n","gs://project_ir_os/multistream6_preprocessed.parquet\n","gs://project_ir_os/multistream7_preprocessed.parquet\n","gs://project_ir_os/multistream8_preprocessed.parquet\n","gs://project_ir_os/multistream9_preprocessed.parquet\n"]}],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'project_ir_os' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and \"multistream\" in b.name and \".parquet\" in b.name:\n","        paths.append(full_path+b.name)\n","        print(full_path+b.name)\n","#     if b.name != 'graphframes.sh':\n","#         paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index for title"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":15,"id":"e4c523e7","metadata":{"id":"e4c523e7","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":null,"id":"82881fbf","metadata":{"id":"82881fbf"},"outputs":[],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"701811af"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":8,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["ls: cannot access 'inverted_index_gcp.py': No such file or directory\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp_title.py"]},{"cell_type":"code","execution_count":10,"id":"57c101a8","metadata":{"id":"57c101a8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp_title.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":11,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp_title import InvertedIndex"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"5540c727"},"source":["**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n","\n","A few notes: \n","1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n","2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n","3. You are not allowed to change any of the code not coming from Colab. "]},{"cell_type":"code","execution_count":16,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","\n","###################### word_count ######################\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","\n","  word_counter = Counter(tokens)\n","  # Work 1 - Bad Time\n","  # ret_list = []\n","  # for token in tokens:\n","  #   if (token not in all_stopwords) and ((token, (id, word_counter.get(token))) not in ret_list):\n","  #     ret_list.append((token, (id, word_counter.get(token))))\n","  # return ret_list\n","\n","  # Work 2 - Best time\n","  return ([(k, (id, v)) for k, v in word_counter.items() if k not in all_stopwords and v > 0 ])\n","\n","\n","#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","###################### doc_normaliztion_count ######################\n","import math\n","\n","def doc_normaliztion_count(text, id):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  docs_tokens = [i for i in tokens if not i in all_stopwords] # query without stop words\n","  word_counter = Counter(docs_tokens)\n","\n","  # calculate (1/|d|)\n","  sum = 0\n","  for term in word_counter:\n","    sum += (word_counter[term] ** 2)\n","  if math.sqrt(sum) == 0:\n","    nf = 1\n","  else:\n","    nf = (1 / math.sqrt(sum))\n","\n","  return (id,nf)\n","#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","###################### reduce_word_counts ######################\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","\n","  return sorted(unsorted_pl, key=lambda x: x[0])\n","\n","\n","###################### calculate_df ######################\n","\n","import enum\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","\n","  #print(postings.map(lambda x: x[1]).take(1))\n","\n","  # Work 1 - Two lines\n","  # RDD_dict = postings.collectAsMap()\n","  # return sc.parallelize([(token, len(RDD_dict[token])) for token in RDD_dict])\n","\n","  # Work 2 - One line!!!\n","  return postings.mapValues(lambda df: len(df))\n","\n","\n","###################### partition_postings_and_write  ######################\n","\n","def partition_postings_and_write(postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","\n","  # work 1 - 2 lines\n","  # RDD_dict = postings.map(lambda token: (token2bucket_id(token[0]), token)).groupByKey()\n","  # #print(RDD_dict.take(1))\n","  # return RDD_dict.map(InvertedIndex.write_a_posting_list)\n","\n","  # Work 2 - one line!!!\n","  return postings.map(lambda token: (token2bucket_id(token[0]), token)).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name))\n"]},{"cell_type":"code","execution_count":17,"id":"SoZx8wp46Ib_","metadata":{"id":"SoZx8wp46Ib_"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["docs_title_counts = doc_title_pairs.map(lambda x: doc_normaliztion_count(x[0], x[1])) # rdd -> normalize docs\n","\n","# collectAsMap collects the results to the master node's memory as a dictionary.\n","# we know it's not so big so this is okay.\n","nf_dict = docs_title_counts.collectAsMap()\n","\n","\n","doc_title_pairs1 = parquetFile.select(\"id\", \"title\").rdd\n","title_dict = doc_title_pairs1.collectAsMap()"]},{"cell_type":"code","execution_count":18,"id":"55c8764e","metadata":{"id":"55c8764e","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","# postings_filtered = postings.filter(lambda x: len(x[1])>50) # TODO: remove for title and anchor\n","w2df = calculate_df(postings)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":19,"id":"jhm2_L6a6-1G","metadata":{"id":"jhm2_L6a6-1G"},"outputs":[{"name":"stdout","output_type":"stream","text":["35.882874727249146\n"]}],"source":["print(index_const_time)"]},{"cell_type":"code","execution_count":20,"id":"ab3296f4","metadata":{"id":"ab3296f4","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp_title'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"f6f66e3a"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":21,"id":"a5d2cfb6","metadata":{"id":"a5d2cfb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index_title.pkl [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","\\ [1 files][321.2 MiB/321.2 MiB]                                                \n","Operation completed over 1 objects/321.2 MiB.                                    \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","\n","#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","inverted.nf = nf_dict\n","inverted.numberOfDocs = len(nf_dict)\n","inverted.TitlesOfDocs = title_dict\n","#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","# write the global stats out\n","inverted.write_index('.', 'index_title') # TODO::\n","# upload to gs\n","index_src = \"index_title.pkl\" #TODO::\n","index_dst = f'gs://{bucket_name}/postings_gcp_title/{index_src}' # TODO::\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":22,"id":"8f880d59","metadata":{"id":"8f880d59","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["321.16 MiB  2023-01-10T12:31:50Z  gs://project_ir_os/postings_gcp_title/index_title.pkl\r\n","TOTAL: 1 objects, 336761357 bytes (321.16 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"96e9a610","metadata":{"id":"96e9a610"},"source":["# Reporting"]},{"cell_type":"markdown","id":"a1da57c7","metadata":{"id":"a1da57c7"},"source":["**YOUR TASK (5 points):** execute and complete the following lines to complete \n","the reporting requirements for assignment #3. "]},{"cell_type":"code","execution_count":null,"id":"0f0d5523","metadata":{"id":"0f0d5523","nbgrader":{"grade":false,"grade_id":"cell-size_ofi_input_data","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"54595c29-4ae3-4b78-86d0-d8457ae9c150"},"outputs":[{"name":"stdout","output_type":"stream","text":["14.28 GiB    gs://wikidata_preprocessed\r\n"]}],"source":["# size of input data\n","!gsutil du -sh \"gs://wikidata_preprocessed/\""]},{"cell_type":"code","execution_count":null,"id":"ce25a98a","metadata":{"id":"ce25a98a","nbgrader":{"grade":false,"grade_id":"cell-size_of_index_data","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"44d9721a-1cd7-4e59-9f78-5439864cfdad"},"outputs":[{"name":"stdout","output_type":"stream","text":["5.93 GiB     gs://sise_ir_assignment3/postings_gcp\r\n"]}],"source":["# size of index data\n","index_dst = f'gs://{bucket_name}/postings_gcp_body/'\n","!gsutil du -sh \"$index_dst\""]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}